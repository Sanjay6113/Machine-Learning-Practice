{"cells":[{"cell_type":"code","source":["# Debug helper: find leakage and verify real generalization\n","!pip install -q scikit-learn==1.2.2 librosa\n","\n","import os, sys\n","import numpy as np\n","import pandas as pd\n","from collections import Counter\n","from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","from sklearn.feature_selection import mutual_info_classif\n","from sklearn.neighbors import NearestNeighbors\n","\n","CSV_PATH = '/content/music_genre_features.csv'   # change if needed\n","print(\"Loading:\", CSV_PATH)\n","df = pd.read_csv(CSV_PATH)\n","print(\"Shape:\", df.shape)\n","display(df.head())\n","\n","# ---- find the feature column the same way you did ----\n","target_col = 'genre'\n","feature_col = None\n","for col in df.columns:\n","    if col == target_col:\n","        continue\n","    if df[col].astype(str).str.contains(',').any():\n","        feature_col = col\n","        break\n","if feature_col is None:\n","    raise ValueError(\"No comma-separated feature column found. Check CSV format.\")\n","print(\"Using feature column:\", feature_col)\n","\n","# ---- expand feature vectors ----\n","def parse_vector(s):\n","    return np.array([float(i) for i in str(s).split(',') if i.strip()!=''])\n","\n","vectors = df[feature_col].apply(parse_vector)\n","X = np.vstack(vectors.values)\n","print(\"Feature matrix shape:\", X.shape)\n","\n","# ---- labels ----\n","le = LabelEncoder()\n","y_int = le.fit_transform(df[target_col].astype(str))\n","print(\"Classes:\", list(le.classes_))\n","print(\"Class counts:\", Counter(y_int))\n","\n","# ---- 1) Duplicate/sample-leak check ----\n","# hash each feature vector (rounded to reduce float noise) and look for duplicates\n","round_dec = 6\n","row_hashes = [hash(tuple(np.round(row, round_dec))) for row in X]\n","df['_row_hash'] = row_hashes\n","dup_groups = df[df.duplicated('_row_hash', keep=False)].sort_values('_row_hash')\n","print(\"\\nTotal duplicated feature-hash rows (equal vectors up to rounding):\", dup_groups.shape[0])\n","if not dup_groups.empty:\n","    print(\"Showing up to 20 duplicate groups (hash + genres):\")\n","    display(dup_groups[[feature_col, target_col, '_row_hash']].head(20))\n","\n","# If duplicates exist across different genres -> strong problem\n","if not dup_groups.empty:\n","    dup_hashes = dup_groups['_row_hash'].unique()\n","    for h in dup_hashes[:10]:\n","        g = dup_groups[dup_groups['_row_hash']==h]\n","        if len(g[target_col].unique()) > 1:\n","            print(f\"Hash {h} maps to multiple genres -> LEAK! Example:\")\n","            display(g[[feature_col, target_col]].head())\n","            break\n","\n","# ---- 2) constant features ----\n","stds = X.std(axis=0)\n","const_idx = np.where(stds == 0)[0]\n","print(\"\\nConstant feature indices (std==0):\", const_idx.tolist())\n","if len(const_idx)>0:\n","    print(\"Consider removing constant features; they cannot help generalization.\")\n","\n","# ---- 3) Features that strongly identify a single class (suspicious) ----\n","# discretize values by rounding then check purity of each discrete value\n","suspicious = []\n","min_support = max(3, int(0.005 * X.shape[0]))  # at least 3 occurrences or 0.5% of data\n","for j in range(X.shape[1]):\n","    vals = np.round(X[:,j], 4)   # discretize\n","    counts = {}\n","    for v, lab in zip(vals, y_int):\n","        counts.setdefault(v, set()).add(lab)\n","    # find discrete values that only occur with a single label and are frequent\n","    for v, labs in counts.items():\n","        if len(labs)==1:\n","            # support = frequency of this discrete value\n","            support = np.sum(vals==v)\n","            if support >= min_support:\n","                suspicious.append((j, v, support, list(labs)[0]))\n","                break\n","# Print suspicious features (index, value, frequency, label)\n","print(\"\\nSuspicious features that have a discrete value mapping only to one label (index, value, freq, label):\")\n","for t in suspicious[:20]:\n","    print(t)\n","if not suspicious:\n","    print(\"None found at the chosen discretization/support thresholds.\")\n","\n","# ---- 4) Mutual information ranking (continuous) ----\n","print(\"\\nComputing mutual information (this may take a moment)...\")\n","mi = mutual_info_classif(X, y_int, discrete_features=False, random_state=42)\n","mi_idx = np.argsort(mi)[::-1]\n","print(\"Top features by mutual information (index: score):\")\n","for idx in mi_idx[:10]:\n","    print(idx, f\"{mi[idx]:.4f}\")\n","\n","# ---- 5) Proper split BEFORE scaling (important) ----\n","X_train_raw, X_test_raw, y_train_int, y_test_int = train_test_split(\n","    X, y_int, test_size=0.2, random_state=42, stratify=y_int\n",")\n","print(\"\\nSplit sizes (raw):\", X_train_raw.shape, X_test_raw.shape)\n","\n","# Scale using train stats ONLY\n","scaler = StandardScaler().fit(X_train_raw)\n","X_train = scaler.transform(X_train_raw)\n","X_test = scaler.transform(X_test_raw)\n","\n","# ---- 6) Simple baseline models to detect \"too-good-to-be-true\" ----\n","print(\"\\nTraining LogisticRegression baseline...\")\n","lr = LogisticRegression(max_iter=2000)\n","lr.fit(X_train, y_train_int)\n","pred_lr = lr.predict(X_test)\n","acc_lr = accuracy_score(y_test_int, pred_lr)\n","print(\"LogisticRegression test accuracy:\", acc_lr)\n","print(classification_report(y_test_int, pred_lr, target_names=le.classes_))\n","\n","print(\"\\nTraining DecisionTree baseline...\")\n","dt = DecisionTreeClassifier()\n","dt.fit(X_train, y_train_int)\n","pred_dt = dt.predict(X_test)\n","acc_dt = accuracy_score(y_test_int, pred_dt)\n","print(\"DecisionTree test accuracy:\", acc_dt)\n","print(classification_report(y_test_int, pred_dt, target_names=le.classes_))\n","\n","# ---- 7) Cross-validation (stratified) for more robust estimate ----\n","print(\"\\n5-fold Stratified CV (LogisticRegression):\")\n","skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","cv_scores = cross_val_score(LogisticRegression(max_iter=2000), scaler.transform(X), y_int, cv=skf, scoring='accuracy')\n","print(\"CV accuracies:\", cv_scores)\n","print(\"CV mean accuracy:\", cv_scores.mean())\n","\n","# ---- 8) Nearest neighbor check helper (useful to compare an uploaded song) ----\n","nn = NearestNeighbors(n_neighbors=1).fit(X_train)\n","def nn_check(vec):\n","    # vec is raw (unscaled) feature vector -> scale it\n","    vec_s = scaler.transform(np.array(vec).reshape(1,-1))\n","    dist, idx = nn.kneighbors(vec_s)\n","    train_idx = idx[0][0]\n","    return float(dist[0][0]), train_idx, y_train_int[train_idx]\n","\n","print(\"\\nNearest-neighbor helper ready: call nn_check(your_vector) after you compute features for an uploaded song.\")\n","print(\"It will return (distance, train_index, train_genre_int). You can then inspect the train sample and its label.\")\n","\n","# ---- Summary suggestions ----\n","print(\"\\n=== QUICK SUGGESTIONS BASED ON ABOVE ===\")\n","if (dup_groups.shape[0] > 0):\n","    print(\"- Found duplicate feature vectors. Make sure the same song (or same feature vector) is NOT in both train and test.\")\n","if len(const_idx)>0:\n","    print(\"- Remove constant features (they shouldn't cause 100% but are useless).\")\n","if suspicious:\n","    print(\"- Found features with discrete values tied only to one label: investigate those columns (they may contain encoded label or id).\")\n","if acc_lr == 1.0 or acc_dt == 1.0:\n","    print(\"- Baseline models achieve perfect accuracy too -> very likely leakage or dataset trivially separable (check for label encoding inside features).\")\n","print(\"- Always split BEFORE fitting scalers or any preprocessing that uses statistics of the whole dataset.\")\n","print(\"- Check that the feature extraction code you use for uploaded songs is IDENTICAL to how the CSV was created.\")\n","print(\"- If your dataset contains multiple frames/segments from the same underlying song, use a grouped split (group by song id) so samples from the same song don't leak between train/test.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":412},"id":"ODFWqi3y5KTz","executionInfo":{"status":"error","timestamp":1760181202754,"user_tz":-330,"elapsed":5168,"user":{"displayName":"Sanjay","userId":"12692044113563509872"}},"outputId":"fc363464-d328-4bb2-af9a-ce055e169f3d"},"id":"ODFWqi3y5KTz","execution_count":24,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1903006050.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmutual_info_classif\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/tree/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m from ._classes import (\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mBaseDecisionTree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m )\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_criterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_splitter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_criterion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCriterion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_splitter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSplitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32msklearn/tree/_criterion.pyx\u001b[0m in \u001b[0;36minit sklearn.tree._criterion\u001b[0;34m()\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":5}