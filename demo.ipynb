{"cells":[{"cell_type":"markdown","metadata":{"id":"Gx_O2uQMW8Il"},"source":["# Song Genre Classification Demo (Colab)"],"id":"Gx_O2uQMW8Il"},{"cell_type":"markdown","metadata":{"id":"lxA5728bW8Im"},"source":["This notebook demonstrates the key functionalities of the Song Genre Classification project in a Google Colab environment."],"id":"lxA5728bW8Im"},{"cell_type":"markdown","metadata":{"id":"rD_LQM74W8Im"},"source":["## 1. Setup and Dependencies"],"id":"rD_LQM74W8Im"},{"cell_type":"markdown","metadata":{"id":"Xr9NP63ZW8In"},"source":["First, we need to clone the repository and install the required libraries from `requirements.txt`."],"id":"Xr9NP63ZW8In"},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wnZbUj53W8In","executionInfo":{"status":"ok","timestamp":1759835697442,"user_tz":-330,"elapsed":219,"user":{"displayName":"Sanjay","userId":"12692044113563509872"}},"outputId":"e9ae5667-102c-4ad9-aa45-1fb708215896"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'song-genre-classification'...\n","warning: You appear to have cloned an empty repository.\n","/content/song-genre-classification/song-genre-classification\n"]}],"source":["# Clone the repository (replace with your repo URL)\n","!git clone https://github.com/itsmskoff-byte/song-genre-classification.git\n","%cd song-genre-classification"],"id":"wnZbUj53W8In"},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hcsctqtRW8In","executionInfo":{"status":"ok","timestamp":1759835950688,"user_tz":-330,"elapsed":3351,"user":{"displayName":"Sanjay","userId":"12692044113563509872"}},"outputId":"e0153aa5-a54e-457b-a9b1-6e4f34c59d31"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n","\u001b[0m/bin/bash: line 1: spleeter: command not found\n"]}],"source":["# Install dependencies\n","!pip install -r requirements.txt\n","# Install Spleeter model (2stems model)\n","!spleeter install"],"id":"hcsctqtRW8In"},{"cell_type":"markdown","metadata":{"id":"DrIjSvCGW8In"},"source":["## 2. Download Sample Data"],"id":"DrIjSvCGW8In"},{"cell_type":"markdown","metadata":{"id":"8yPFOI_RW8Io"},"source":["Download a few sample audio files from different genres for demonstration."],"id":"8yPFOI_RW8Io"},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qXRd0NnTW8Io","executionInfo":{"status":"ok","timestamp":1759835700520,"user_tz":-330,"elapsed":114,"user":{"displayName":"Sanjay","userId":"12692044113563509872"}},"outputId":"3478ca7e-a211-4424-9c6d-601bc39f2321"},"outputs":[{"output_type":"stream","name":"stdout","text":["Placeholder for downloading sample audio files. Please replace with actual download commands.\n"]}],"source":["# Create a directory for sample data\n","!mkdir -p sample_data/blues sample_data/classical\n","\n","# Download sample audio files (replace with actual public URLs to small audio files)\n","# Example: Using dummy files or small snippets from a public dataset like Free Music Archive (FMA)\n","# Note: You'll need to find actual downloadable links for diverse genres.\n","# For demonstration purposes, let's create dummy files or download placeholders if real ones aren't readily available.\n","\n","# Example using wget (replace URLs)\n","# !wget -O sample_data/blues/sample_blues.mp3 https://example.com/sample_blues.mp3\n","# !wget -O sample_data/classical/sample_classical.wav https://example.com/sample_classical.wav\n","\n","# Placeholder code if public URLs are hard to find for a demo\n","print(\"Placeholder for downloading sample audio files. Please replace with actual download commands.\")\n","# Create dummy audio files for structure testing if needed\n","# !echo \"dummy audio content\" > sample_data/blues/dummy_blues.wav\n","# !echo \"dummy audio content\" > sample_data/classical/dummy_classical.wav"],"id":"qXRd0NnTW8Io"},{"cell_type":"markdown","metadata":{"id":"96MLmuJiW8Io"},"source":["## 3. Audio Processing and Feature Extraction"],"id":"96MLmuJiW8Io"},{"cell_type":"markdown","metadata":{"id":"AAC_p6b9W8Io"},"source":["Demonstrate using the `utils.py` functions to load audio, perform source separation, and extract features."],"id":"AAC_p6b9W8Io"},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":403},"id":"MMDDHaCkW8Io","executionInfo":{"status":"error","timestamp":1759835966254,"user_tz":-330,"elapsed":70,"user":{"displayName":"Sanjay","userId":"12692044113563509872"}},"outputId":"22b17859-6170-461b-cc53-c9e2f83bc978"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'utils'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2674556751.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import utility functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Ensure utils.py is in the project root or accessible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparate_stems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msliding_window\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_classical_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_mel_spectrogram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Define a sample audio file path (replace with the actual path after downloading)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["# Import utility functions\n","# Ensure utils.py is in the project root or accessible\n","from utils import load_audio, separate_stems, sliding_window, extract_classical_features, extract_mel_spectrogram\n","\n","# Define a sample audio file path (replace with the actual path after downloading)\n","# Assuming you downloaded sample_data/blues/sample_blues.mp3\n","sample_audio_path = 'sample_data/blues/sample_blues.mp3'\n","\n","# Check if the sample file exists (important for Colab demo)\n","import os\n","if not os.path.exists(sample_audio_path):\n","    print(f\"Sample audio file not found at {sample_audio_path}. Please download sample data first.\")\n","else:\n","    print(f\"Processing sample audio: {sample_audio_path}\")\n","\n","    # --- Load Audio ---\n","    st.markdown(\"### Loading Audio\")\n","    audio_data, sample_rate = load_audio(sample_audio_path)\n","\n","    if audio_data is not None:\n","        st.info(f\"Audio loaded successfully. Sample rate: {sample_rate}, Duration: {len(audio_data)/sample_rate:.2f} seconds\")\n","\n","        # --- Source Separation ---\n","        st.markdown(\"### Source Separation\")\n","        separated_output_dir = 'demo_separated_stems'\n","        vocal_path, instrumental_path = separate_stems(sample_audio_path, output_dir=separated_output_dir)\n","\n","        if vocal_path and instrumental_path:\n","            st.success(f\"Stems saved to {separated_output_dir}\")\n","            st.audio(vocal_path, format='audio/wav', caption='Vocal Stem')\n","            st.audio(instrumental_path, format='audio/wav', caption='Instrumental Stem')\n","        else:\n","            st.warning(\"Source separation failed.\")\n","\n","        # --- Feature Extraction (Classical) ---\n","        st.markdown(\"### Classical Feature Extraction\")\n","        # Use a small segment for demonstration\n","        window_size_sec = 3.0\n","        hop_length_sec = 3.0 # Use same hop for single segment demo\n","        segments = list(sliding_window(audio_data, sample_rate, window_size_sec, hop_length_sec))\n","\n","        if segments:\n","            first_segment = segments[0]\n","            classical_features = extract_classical_features(first_segment, sample_rate)\n","            if classical_features is not None:\n","                st.info(f\"Extracted {len(classical_features)} classical features from the first segment.\")\n","                st.write(\"Sample Classical Features (first 5):\", classical_features[:5])\n","            else:\n","                st.warning(\"Classical feature extraction failed.\")\n","        else:\n","            st.warning(\"Audio is too short for segmentation.\")\n","\n","        # --- Feature Extraction (Deep Learning) ---\n","        st.markdown(\"### Deep Learning Feature Extraction\")\n","        if segments:\n","            first_segment = segments[0]\n","            mel_spectrogram = extract_mel_spectrogram(first_segment, sample_rate)\n","            if mel_spectrogram is not None:\n","                st.info(f\"Extracted Mel-Spectrogram with shape: {mel_spectrogram.shape}\")\n","                # Displaying the spectrogram image directly in Colab might require matplotlib or similar\n","                # For simplicity, just show shape and a snippet\n","                st.write(\"Sample Mel-Spectrogram snippet:\")\n","                st.image(mel_spectrogram[:50, :50], caption=\"Mel-Spectrogram Snippet\") # Display as image\n","            else:\n","                st.warning(\"Deep learning feature extraction failed.\")\n","        else:\n","             st.warning(\"Audio is too short for segmentation.\")\n","\n","    else:\n","        st.error(\"Failed to load audio data.\")"],"id":"MMDDHaCkW8Io"},{"cell_type":"markdown","metadata":{"id":"y3giwGGbW8Ip"},"source":["## 4. Load Pre-trained Models and Label Encoder"],"id":"y3giwGGbW8Ip"},{"cell_type":"markdown","metadata":{"id":"o6hhzuL9W8Ip"},"source":["Assuming models (`classical_model.joblib`, `deep_learning_model.h5`) and the label encoder (`label_encoder.pkl`) are available (e.g., from training or a pre-trained download)."],"id":"o6hhzuL9W8Ip"},{"cell_type":"code","execution_count":null,"metadata":{"id":"DIr_bXdDW8Ip","executionInfo":{"status":"aborted","timestamp":1759835700645,"user_tz":-330,"elapsed":3692,"user":{"displayName":"Sanjay","userId":"12692044113563509872"}}},"outputs":[],"source":["import pickle\n","import joblib\n","import tensorflow as tf\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Define paths (replace with actual paths if different)\n","classical_model_path = 'trained_models/classical_model.joblib'\n","deep_learning_model_path = 'trained_models/deep_learning_model.h5'\n","label_encoder_path = 'trained_models/label_encoder.pkl'\n","\n","# --- Load Classical Model ---\n","st.markdown(\"### Loading Classical Model\")\n","classical_model = None\n","if os.path.exists(classical_model_path):\n","    try:\n","        classical_model = joblib.load(classical_model_path)\n","        st.success(\"Classical model loaded successfully.\")\n","    except Exception as e:\n","        st.error(f\"Error loading classical model: {e}\")\n","else:\n","    st.warning(f\"Classical model not found at {classical_model_path}. Please train it first or provide the correct path.\")\n","\n","# --- Load Deep Learning Model ---\n","st.markdown(\"### Loading Deep Learning Model\")\n","deep_learning_model = None\n","if os.path.exists(deep_learning_model_path):\n","    try:\n","        deep_learning_model = tf.keras.models.load_model(deep_learning_model_path, compile=False)\n","        st.success(\"Deep learning model loaded successfully.\")\n","    except Exception as e:\n","        st.error(f\"Error loading deep learning model: {e}\")\n","else:\n","    st.warning(f\"Deep learning model not found at {deep_learning_model_path}. Please train it first or provide the correct path.\")\n","\n","# --- Load Label Encoder ---\n","st.markdown(\"### Loading Label Encoder\")\n","label_encoder = None\n","if os.path.exists(label_encoder_path):\n","    try:\n","        with open(label_encoder_path, 'rb') as f:\n","            label_encoder = pickle.load(f)\n","        st.success(\"Label encoder loaded successfully.\")\n","        st.info(f\"Genres: {list(label_encoder.classes_)}\")\n","    except Exception as e:\n","        st.error(f\"Error loading label encoder: {e}\")\n","else:\n","    st.warning(f\"Label encoder not found at {label_encoder_path}. Please train a model first to generate it or provide the correct path.\")"],"id":"DIr_bXdDW8Ip"},{"cell_type":"markdown","metadata":{"id":"r1EgFeqGW8Ip"},"source":["## 5. Perform Inference"],"id":"r1EgFeqGW8Ip"},{"cell_type":"markdown","metadata":{"id":"OsC0CyOrW8Ip"},"source":["Use the loaded models and the prediction logic (similar to `predict.py`) to classify the sample audio file."],"id":"OsC0CyOrW8Ip"},{"cell_type":"code","execution_count":null,"metadata":{"id":"88uy0XvSW8Ip","executionInfo":{"status":"aborted","timestamp":1759835700771,"user_tz":-330,"elapsed":1,"user":{"displayName":"Sanjay","userId":"12692044113563509872"}}},"outputs":[],"source":["# Assuming sample_audio_path, audio_data, and sample_rate are available from Section 3\n","# Assuming classical_model, deep_learning_model, and label_encoder are available from Section 4\n","\n","# Define segment parameters (must match training)\n","WINDOW_SIZE_SEC = 3.0\n","HOP_LENGTH_SEC = 1.5\n","TARGET_SR = 22050\n","N_MELS = 128\n","\n","def predict_song(audio_data, sample_rate, model, model_type, feature_type, label_encoder):\n","    \"\"\"\n","    Performs prediction on a full audio data array.\n","    Simplified prediction logic for the demo notebook.\n","    \"\"\"\n","    segment_predictions = []\n","\n","    # Get input shape for deep learning model if applicable\n","    input_shape = None\n","    if model_type == 'deep_learning' and model:\n","        try:\n","            if hasattr(model, 'input_shape') and len(model.input_shape) > 1:\n","                 input_shape = model.input_shape[1:]\n","            elif model.layers and hasattr(model.layers[0], 'input_shape') and len(model.layers[0].input_shape) > 1: # For Functional API models\n","                 input_shape = model.layers[0].input_shape[0][1:]\n","            else:\n","                print(\"Warning: Could not determine input shape from deep learning model.\")\n","        except Exception as e:\n","            print(f\"Error determining deep learning input shape: {e}\")\n","\n","    # Apply sliding window and collect segment predictions\n","    segments_generator = sliding_window(audio_data, sample_rate, WINDOW_SIZE_SEC, HOP_LENGTH_SEC)\n","    for i, segment in enumerate(segments_generator):\n","        try:\n","            if feature_type == 'classical':\n","                features = extract_classical_features(segment, sample_rate)\n","                if features is None:\n","                    print(f\"Warning: Feature extraction failed for segment {i+1}.\")\n","                    continue\n","                features = features.reshape(1, -1)\n","\n","            elif feature_type == 'deep_learning':\n","                features = extract_mel_spectrogram(segment, sample_rate, n_mels=N_MELS)\n","                if features is None:\n","                    print(f\"Warning: Feature extraction failed for segment {i+1}.\")\n","                    continue\n","                features = np.expand_dims(features, axis=-1)\n","                features = np.expand_dims(features, axis=0)\n","\n","                if input_shape is not None and features.shape[1:] != input_shape:\n","                     print(f\"Warning: Extracted feature shape {features.shape[1:]} does not match expected input shape {input_shape} for segment {i+1}. Skipping.\")\n","                     continue\n","\n","            else:\n","                print(f\"Error: Unsupported feature type: {feature_type}\")\n","                return None, None\n","\n","            # Perform prediction on the segment\n","            if model_type == 'classical':\n","                 if hasattr(model, 'predict_proba'):\n","                      prediction_probs = model.predict_proba(features)\n","                 else:\n","                      print(\"Warning: Classical model lacks predict_proba. Using predict.\")\n","                      # Fallback: predict class and create dummy probability array\n","                      predicted_class_idx = model.predict(features)[0]\n","                      num_classes = len(label_encoder.classes_)\n","                      prediction_probs = np.zeros((1, num_classes))\n","                      prediction_probs[0, predicted_class_idx] = 1.0\n","\n","\n","            elif model_type == 'deep_learning':\n","                prediction_probs = model.predict(features)\n","\n","            else:\n","                 print(f\"Error: Unsupported model type: {model_type}\")\n","                 return None, None\n","\n","            segment_predictions.append(prediction_probs[0])\n","        except Exception as e:\n","            print(f\"Error processing segment {i+1}: {e}\")\n","            continue\n","\n","    # Aggregate predictions\n","    if not segment_predictions:\n","        print(\"No successful segment predictions.\")\n","        return None, None\n","\n","    all_predictions = np.vstack(segment_predictions)\n","    average_probabilities = np.mean(all_predictions, axis=0)\n","    predicted_class_index = np.argmax(average_probabilities)\n","    predicted_genre = label_encoder.inverse_transform([predicted_class_index])[0]\n","\n","    return predicted_genre, average_probabilities\n","\n","# --- Perform Prediction with Classical Model ---\n","st.markdown(\"### Prediction with Classical Model\")\n","if classical_model and label_encoder and audio_data is not None:\n","    st.info(\"Classifying sample audio using Classical Model...\")\n","    predicted_genre_classical, probs_classical = predict_song(audio_data, sample_rate, classical_model, 'classical', 'classical', label_encoder)\n","\n","    if predicted_genre_classical:\n","        st.success(f\"Classical Model Predicted Genre: **{predicted_genre_classical}**\")\n","        st.write(\"Probabilities:\")\n","        genre_probs = list(zip(label_encoder.classes_, probs_classical))\n","        genre_probs_sorted = sorted(genre_probs, key=lambda item: item[1], reverse=True)\n","        for genre, prob in genre_probs_sorted:\n","            st.text(f\"  {genre}: {prob:.4f}\")\n","    else:\n","        st.error(\"Classical model prediction failed.\")\n","elif audio_data is None:\n","    st.warning(\"Audio data not loaded. Cannot run classical prediction demo.\")\n","else:\n","    st.warning(\"Classical model or label encoder not loaded. Cannot run classical prediction demo.\")\n","\n","# --- Perform Prediction with Deep Learning Model ---\n","st.markdown(\"### Prediction with Deep Learning Model\")\n","# Note: Ensure the deep learning model was trained with deep learning features (mel-spectrograms)\n","if deep_learning_model and label_encoder and audio_data is not None:\n","    st.info(\"Classifying sample audio using Deep Learning Model...\")\n","    # Need to determine input shape for deep learning model\n","    predicted_genre_dl, probs_dl = predict_song(audio_data, sample_rate, deep_learning_model, 'deep_learning', 'deep_learning', label_encoder)\n","\n","    if predicted_genre_dl:\n","        st.success(f\"Deep Learning Model Predicted Genre: **{predicted_genre_dl}**\")\n","        st.write(\"Probabilities:\")\n","        genre_probs = list(zip(label_encoder.classes_, probs_dl))\n","        genre_probs_sorted = sorted(genre_probs, key=lambda item: item[1], reverse=True)\n","        for genre, prob in genre_probs_sorted:\n","            st.text(f\"  {genre}: {prob:.4f}\")\n","    else:\n","        st.error(\"Deep learning model prediction failed.\")\n","elif audio_data is None:\n","     st.warning(\"Audio data not loaded. Cannot run deep learning prediction demo.\")\n","else:\n","    st.warning(\"Deep learning model or label encoder not loaded. Cannot run deep learning prediction demo.\")"],"id":"88uy0XvSW8Ip"},{"cell_type":"markdown","metadata":{"id":"CtcpVPa0W8Iq"},"source":["## 6. Running `train.py` and `app.py` in Colab"],"id":"CtcpVPa0W8Iq"},{"cell_type":"markdown","metadata":{"id":"ceQrrGDbW8Iq"},"source":["While this notebook demonstrates key components, the full training and web application are typically run as separate scripts. Here's how you would execute them in Colab:"],"id":"ceQrrGDbW8Iq"},{"cell_type":"markdown","metadata":{"id":"ZWzWfMJeW8Iq"},"source":["### Running `train.py`"],"id":"ZWzWfMJeW8Iq"},{"cell_type":"markdown","metadata":{"id":"NUPJVqNrW8Iq"},"source":["You would first need to prepare your data as described in the `README.md` and the data preparation steps (Sections 2 & 3 of this notebook demonstrate parts of this). Assuming you have processed data (e.g., `processed_features.pkl`), you can train a model:"],"id":"NUPJVqNrW8Iq"},{"cell_type":"code","execution_count":null,"metadata":{"id":"nSurgwiHW8Iq","executionInfo":{"status":"aborted","timestamp":1759835700785,"user_tz":-330,"elapsed":3,"user":{"displayName":"Sanjay","userId":"12692044113563509872"}}},"outputs":[],"source":["# Example command to train a classical model\n","# Replace 'path/to/your/processed_data.pkl' with the actual path\n","# Ensure you have prepared data before running this.\n","# !python train.py --model_type classical --data_path path/to/your/processed_data.pkl --save_dir trained_models\n","\n","# Example command to train a deep learning model\n","# Replace 'path/to/your/processed_data_mel.npy' with the actual path to mel-spectrogram data\n","# Ensure you have prepared data before running this.\n","# !python train.py --model_type deep_learning --data_path path/to/your/processed_data_mel.npy --save_dir trained_models\n","\n","print(\"Uncomment the above lines to run training commands in Colab.\")"],"id":"nSurgwiHW8Iq"},{"cell_type":"markdown","metadata":{"id":"10sYDYp9W8Ir"},"source":["### Running `app.py` (Streamlit Web App)"],"id":"10sYDYp9W8Ir"},{"cell_type":"markdown","metadata":{"id":"1CdDpx-pW8Ir"},"source":["To run the Streamlit app in Colab, you typically need to use `ngrok` or a similar service to expose the local server."],"id":"1CdDpx-pW8Ir"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ka6LqGBjW8Ir","executionInfo":{"status":"aborted","timestamp":1759835700787,"user_tz":-330,"elapsed":3,"user":{"displayName":"Sanjay","userId":"12692044113563509872"}}},"outputs":[],"source":["# Install ngrok\n","!pip install ngrok\n","\n","# Run Streamlit app and expose it via ngrok\n","# This will output a public URL to access the app.\n","# Note: This will block the notebook execution until the app is stopped.\n","# !streamlit run app.py & npx ngrok http 8501 --log=stdout > ngrok.log &\n","# import time\n","# time.sleep(5) # Give ngrok time to start\n","# !grep -o 'https://[^ ]*.ngrok.io' ngrok.log || echo \"ngrok URL not found. Check ngrok.log\""],"id":"ka6LqGBjW8Ir"},{"cell_type":"markdown","metadata":{"id":"QI2_ysxKW8Ir"},"source":["**Note:** Running `!streamlit run app.py` directly in a Colab cell might not display the app correctly within the notebook output. Using `ngrok` is the standard way to access Streamlit apps hosted in Colab."],"id":"QI2_ysxKW8Ir"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}